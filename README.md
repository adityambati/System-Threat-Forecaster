# ğŸ”¥ System Threat Detector - Malware Prediction  

![Project Status](https://img.shields.io/badge/Status-Active-green)  
![Contributors](https://img.shields.io/badge/Contributors-1-blue)  
![License](https://img.shields.io/badge/License-MIT-lightgrey)  

## ğŸ” Overview  

This project is part of a Kaggle competition where the goal is to **predict a systemâ€™s probability of getting infected by various families of malware** based on different system properties.  

The dataset is derived from **antivirus telemetry data** containing system configurations, installed security products, OS details, and past malware detections.  

Each row in the dataset represents a **unique machine** identified by `MachineID`, with `target` as the ground truth (1 = Malware detected, 0 = No Malware detected).  

## ğŸ“‚ Repository Structure  
```plaintext
ğŸ“¦ System_Threat_Detector â”œâ”€â”€ ğŸ“‚ data/ # Contains train and test datasets â”‚ â”œâ”€â”€ train.csv
â”‚ â”œâ”€â”€ test.csv
â”œâ”€â”€ ğŸ“œ kaggle_notebook.ipynb # Main Jupyter Notebook with all code
â”œâ”€â”€ ğŸ“œ requirements.txt # Dependencies for running the notebook
â”œâ”€â”€ ğŸ“œ README.md # Project documentation
```

## ğŸš€ Features Implemented  

âœ”ï¸ **Data Cleaning & Preprocessing** â€“ Handling missing values, feature encoding, and feature scaling.  
âœ”ï¸ **Exploratory Data Analysis (EDA)** â€“ Understanding data distributions, correlations, and system security trends.  
âœ”ï¸ **Feature Engineering** â€“ Creating meaningful features to improve model accuracy.  
âœ”ï¸ **Model Training & Evaluation** â€“ Using **XGBoost, LightGBM, Random Forest, and Logistic Regression**.  
âœ”ï¸ **Hyperparameter Tuning** â€“ Optimizing model performance using **GridSearchCV**.  
âœ”ï¸ **Final Predictions** â€“ Generating probability scores for malware infections on test machines.  

## ğŸ“Š Model Performance  

| Model          | Accuracy | F1 Score |
|---------------|---------|----------|
| Logistic Regression | 0.6122 | 0.63 |
| Decision Tree | 0.5568 | 0.56 |
| Random Forest | 0.6064 | 0.62 |
| XGBoost | 0.6254 | 0.64 |
| LightGBM | 0.6287 | 0.65 |

âœ… **Best Model:** `XGBoost` achieved the highest accuracy & F1-score.

## ğŸ” Dataset Details  

The dataset consists of **76 features**, including:  

- **System Information**: `OSVersion`, `Processor`, `RAM`, `DiskCapacity`, `PrimaryDisplayResolution`, `ChassisType`, etc.  
- **Security Features**: `IsSystemProtected`, `FirewallEnabled`, `IsSecureBootEnabled`, `AutoSampleSubmissionEnabled`, etc.  
- **Software & Updates**: `EngineVersion`, `AppVersion`, `SignatureVersion`, `OSBuildLab`, `LicenseActivationChannel`, etc.  
- **Malware Detection History**: `DateAS` (Malware signature dates), `DateOS` (OS update timestamps), and `target`.  

ğŸ“Œ **Full column descriptions can be found in the notebook.**  

## ğŸ“¦ Installation & Usage  

To **run this project locally**, follow these steps:

### Clone the Repository  
```bash
git clone https://github.com/adityambati/System-Threat-Forecaster.git
cd System-Threat-Forecaster
pip install -r requirements.txt\
```
You can run the kaggle_notebook.ipynb using Jupyter Notebook or Google Colab.
